{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text Mgmt Project Review Summarizer_v3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OnKgOm_CVeBd"},"source":["# AI6122 Text Management Review Summarizer\n","Task List:\n","- [ ] Define and justify what a summarizer is. E.g.\n","  + a list of keywords\n","  + a list of key phrases\n","  + a list of noun-adjective pairs \n","  + a list of nounPhrase - adjectivePhrase pairs \n","  + a list of representative sentences \n","- [ ] Technical challenges to achieve ideal summarization and your solution.\n","- [ ] Justify approach is best option for each component in your solution.\n","- [ ] Justify limitations to your approach.\n","- [ ] Evaluate solution with possible alternative solutions (baselines).\n","- [ ] Randomly choose 3 products to create product review summary.\n","\n","Possible algorithms to try:\n","- [x] (Extractive) Word frequency sentence extraction: score sentences by word freq\n","- (Extractive) Text rank algorithm: uses cosine similarity\n","- (Extractive/ Abstractive) Machine learning \n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2n5gRTPjH7K9"},"source":["## Colab Configuration"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1585739344239,"user_tz":-480,"elapsed":1151,"user":{"displayName":"Mengxuan Tan","photoUrl":"","userId":"14268668782165182665"}},"id":"ZMq2SgGeHWzi","outputId":"7682b49b-29a9-4c5e-f3cc-1e035630e0f9","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kT94aIx5Mgux","colab_type":"code","outputId":"961fd6df-ebae-4bf2-dbf9-5c6e35bd4a0d","executionInfo":{"status":"ok","timestamp":1585739350853,"user_tz":-480,"elapsed":4474,"user":{"displayName":"Mengxuan Tan","photoUrl":"","userId":"14268668782165182665"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os \n","os.chdir('/content/drive/My Drive/Colab_Notebooks/text_management/SourceCode/02 Review Summarizer/')\n","!pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab_Notebooks/text_management/SourceCode/02 Review Summarizer\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5Ig1AgUZH2BK"},"source":["## Import Modules & Configurations"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pkJYld8AHfxu","colab":{}},"source":["from collections import OrderedDict\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","import pandas as pd\n","import multiprocessing as mp\n","import datetime\n","from helpers.duallogger import loggersetup\n","from helpers.filehelper import is_not_empty_file_exists, write_to_file, load_from_file\n","import logging\n","\n","from nltk.corpus import stopwords\n","import nltk\n","import re\n","import heapq\n","import collections\n","import operator"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1585739357606,"user_tz":-480,"elapsed":1149,"user":{"displayName":"Mengxuan Tan","photoUrl":"","userId":"14268668782165182665"}},"id":"E7PrOkqt7zMB","outputId":"56144495-f7dc-4df4-b871-0d62e98a6b1d","colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","stopwords_nltk = stopwords.words('english')\n","stopwords_spacy = list(STOP_WORDS)\n","stopwords_spacy.append('\\n')\n","stopwords = stopwords_nltk + list(set(stopwords_spacy) - set(stopwords_nltk))\n","\n","print(\"sw nltk: \", len(stopwords_nltk))\n","print(\"sw spacy: \", len(stopwords_spacy))\n","print(\"combined: \", len(stopwords))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","sw nltk:  179\n","sw spacy:  327\n","combined:  383\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1585739359127,"user_tz":-480,"elapsed":708,"user":{"displayName":"Mengxuan Tan","photoUrl":"","userId":"14268668782165182665"}},"id":"nCmKa36ZHx7v","outputId":"199056f5-e286-4083-a837-1757e1b0c430","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["cores = mp.cpu_count()\n","print(\"Cores:\", cores)\n","gpu = spacy.prefer_gpu()\n","print(\"GPU:\", gpu)\n","\n","log_dir = './logs/'\n","log = loggersetup(log_dir, stdout_level=logging.DEBUG, file_level=logging.DEBUG)\n","\n","# log.debug('Debug message, should only appear in the file.')\n","# log.info('Info message, should appear in file and stdout.')\n","# log.warning('Warning message, should appear in file and stdout.')\n","# log.error('Error message, should appear in file and stdout.')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Cores: 2\n","GPU: False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PCe36p0kH-TB","colab":{}},"source":["parameters = OrderedDict()\n","parameters['json_file'] = 'CellPhoneReview.json'\n","parameters['reload_prod_reviews'] = True\n","parameters['prod_reviews_path'] = './data/prod_reviews.data'\n","parameters['clean_reviews'] = True\n","parameters['reload_clean_reviews'] = True\n","parameters['cleaned_reviews_path'] = './data/prod_reviews_cleaned.data'\n","parameters['reload_collection_frequencies'] = True\n","parameters['collection_frequencies_path'] = './data/word_doc_frequencies.data'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vqJyOITIH_JB"},"source":["## Data Preprocessing\n","\n","* convert_lower_case(data)\n","* lemma(data)\n","* remove_punctuation(data)\n","* remove_stop_words(data)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3octqSUeIQce","colab":{}},"source":["# if not parameters['reload_prod_reviews'] or not is_not_empty_file_exists(parameters['prod_reviews_path']):\n","#     data = pd.read_json(parameters['json_file'], lines = True)\n","#     prod_reviews = data.groupby(['asin'])['reviewText'].apply(' '.join).reset_index()\n","#     log.info(\"Writing prod_reviews to %s\" % parameters['prod_reviews_path'])\n","#     write_to_file(parameters['prod_reviews_path'], prod_reviews)\n","# else:\n","#     log.info(\"Reloading prod_reviews from %s\" % parameters['prod_reviews_path'])\n","#     prod_reviews = load_from_file(parameters['prod_reviews_path'])\n","\n","# pd.set_option('display.max_rows', None)\n","# pd.set_option('display.max_columns', None)\n","# pd.set_option('display.width', None)\n","# pd.set_option('display.max_colwidth', -1)\n","# prod_reviews.head(3) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1585739383824,"user_tz":-480,"elapsed":1532,"user":{"displayName":"Mengxuan Tan","photoUrl":"","userId":"14268668782165182665"}},"id":"P6hptM4A1S7K","outputId":"5a4572d0-9d4e-4ee1-cfdf-632a11c3375d","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Create the nlp object\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# def set_custom_boundaries(doc):\n","#     for i, token in enumerate(doc):\n","#         if token.text in (\"but\"):\n","#             doc[i].is_sent_start = False\n","#         if token.text in (\"’s\", \"'s\"):\n","#             doc[i].is_sent_start = False\n","#         elif token.text in (\"“\", \"‘\") and i < len(doc) - 1:\n","#             # opening quote\n","#             doc[i+1].is_sent_start = False\n","#         elif token.text in (\"”\", \"’\"):\n","#             # closing quote\n","#             doc[i].is_sent_start = False\n","#     return doc\n","\n","# nlp.add_pipe(set_custom_boundaries, before=\"parser\")\n","\n","print(nlp.pipe_names)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["['tagger', 'parser', 'ner']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KjxXiQvnzS9p","colab":{}},"source":["# sentnlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner', 'tagger'])\n","# sentnlp.add_pipe(nlp.create_pipe('sentencizer'))\n","\n","# # Define function to cleanup text by removing personal pronouns, stopwords, and punctuations\n","# def cleanup_text(text, stopwords, punc):\n","#     texts = []\n","#     global sentnlp\n","#     doc = sentnlp(text) # only do tokenization and pos tagging\n","#     for sentence in doc.sents:\n","#         sen_text = nlp(sentence.text, disable=['parser', 'ner'])\n","#         tokens = [tok.lemma_.lower().strip() for tok in sen_text if tok.lemma_ != '-PRON-']\n","#         tokens = [tok for tok in tokens if tok not in stopwords and tok not in punc]\n","#         tokens = ' '.join(tokens)\n","#         texts.append(tokens)\n","#     return str(texts)\n","\n","# def tokenize_sentences(text): \n","#     texts = []\n","#     global sentnlp\n","#     doc = sentnlp(text) # only do tokenization and pos tagging\n","#     for idx, sentence in enumerate(doc.sents):\n","#         texts.append(sentence.text)\n","#     return str(texts)\n","\n","# if parameters['clean_reviews']:\n","#     if not parameters['reload_clean_reviews'] or not is_not_empty_file_exists(parameters['cleaned_reviews_path']):\n","#         punctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~©'\n","#         log.debug(\"reviews to be stripped away with stopwords and punctation\")\n","#         prod_reviews['processedSentences'] = prod_reviews['reviewText'].apply(lambda x: cleanup_text(x, stopwords, punctuations))\n","#         prod_reviews['reviewSentences'] = prod_reviews['reviewText'].apply(lambda x: tokenize_sentences(x))\n","#         log.info(\"Writing cleaned reviews to %s\" % parameters['cleaned_reviews_path'])\n","#         write_to_file(parameters['cleaned_reviews_path'], prod_reviews)\n","#     else:\n","#         log.info(\"Reloading cleaned reviews from %s\" % parameters['cleaned_reviews_path'])\n","#         prod_reviews = load_from_file(parameters['cleaned_reviews_path'])\n","        \n","# else:\n","#     log.info(\"unprocessed reviews will be used\")\n","#     prod_reviews['reviewSentences'] = prod_reviews['reviewText'].apply(lambda x: tokenize_sentences(x))\n","#     prod_reviews['processedSentences'] = prod_reviews['reviewSentences']\n","\n","# print(prod_reviews['processedSentences'][7])\n","# print(prod_reviews['reviewSentences'][7])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"frmbiCw5VSft","colab_type":"code","colab":{}},"source":["# load cleaned data from file\n","prod_reviews = load_from_file(parameters['cleaned_reviews_path'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1YXm_mfpVYA3","colab_type":"code","outputId":"eb23ac08-6978-4ef8-f225-82e1076e52eb","executionInfo":{"status":"ok","timestamp":1585739412181,"user_tz":-480,"elapsed":1259,"user":{"displayName":"Mengxuan Tan","photoUrl":"","userId":"14268668782165182665"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["print(prod_reviews['processedSentences'][7])\n","print(prod_reviews['reviewSentences'][7])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["['great product order future .', 'money spend', 'need great car charger need', 'light port little rough charge phone fine .', 'wish little reach use phone passenger seat car comfortably big problem .', 'reach pretty far coil section stress charge port phone people realize eventually break .', 'purchase charger sister girlfriend .', 'samsung galaxy phone need car charger oem fanboy swoop couple remain slightly skeptical actually mail price .', 'happy report definitely receive 2 oem samsung car charger mail seal bag barcode kind normally receive new refurb phone .', 'wrong price certainly argue benefit oem vs. generic output reliability list .', 'buy samsung charge .', 'work perfectly build solidly 20 verizon charger .', 'highly recommend car charger thank great price', 'need pleased .', 'buy cheap lame .', 'charger work great .', 'issue .', 'wish retractable cord like charger use love .']\n","['This is another great product and i will always order from here in the future.', 'money spent well!!', 'If you need a great car charger this is the one you need!!', 'goes into the lighter port a little rough but it charges the phone fine.', 'I wish it had a little more reach so i could use the phone from the passenger seat of the car more comfortably but its not a big problem.', 'it will reach pretty far because of the coiled section but this will put stress on the charge port of your phone which many people dont realize will eventually break it.', 'I purchased two of these chargers for my sister and girlfriend.', 'They both have Samsung Galaxy phones and needed a car charger so being the OEM fanboy I am, I swooped up on a couple of these for them while remaining slightly skeptical of what was actually going to show up in the mail at this price.', \"I'm happy to report I definitely received 2 OEM Samsung car chargers in the mail in sealed bags with part # barcodes (the kind you'd normally receive in a new/refurb phone).\", \"Can't go wrong with the price and certainly can't argue with the benefits of OEM vs. generic (mA output and reliability at the top of the list!).\", 'I bought this for my Samsung Charge.', 'It works perfectly and is built as solidly as the $20 Verizon charger.', 'I highly recommend this car charger and thanks for the great price!', 'It does what it needs to do, very pleased.', 'I bought some other cheap lame one and will never go back.', 'Charger works great.', \"Haven't had any issues with it at all.\", \"I wish it has a retractable cord like some other chargers I've used, but other than that, I love it.\"]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OCcd3bnzVVer"},"source":["## Review Summarizer"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MzJB0PQ4nMN2"},"source":["### Collection Frequency"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1585739434470,"user_tz":-480,"elapsed":5378,"user":{"displayName":"Mengxuan Tan","photoUrl":"","userId":"14268668782165182665"}},"id":"FGaPJrCFnOjx","outputId":"9711da40-9154-4dec-aa2d-ac98b0a3c4ec","colab":{"base_uri":"https://localhost:8080/","height":89,"output_embedded_package_id":"12P_aUR3cvG2rwpJSAWnDU96ZCeWqKHQb"}},"source":["import numpy as np\n","def collection_frequencies(prod_reviews, use_cleaned) -> dict:\n","    collection_frequencies = {}\n","    for index, row in prod_reviews.iterrows():\n","        seen_tokens = set()\n","        if (use_cleaned):\n","            reviews = ' '.join(str(x) for x in eval(row['processedSentences'])) \n","        else:\n","            reviews = ' '.join(str(x) for x in eval(row['reviewSentences'])) \n","        for token in nlp.make_doc(reviews):   #tokenize\n","            if token.text in seen_tokens:\n","                continue\n","            if token.text not in stopwords:\n","                if token.text not in collection_frequencies.keys():\n","                    collection_frequencies[token.text] = 1\n","                else:\n","                    collection_frequencies[token.text] += 1\n","            seen_tokens.add(token.text)\n","    return collection_frequencies\n","\n","if not parameters['reload_collection_frequencies'] or not is_not_empty_file_exists(parameters['collection_frequencies_path']):\n","    word_doc_frequencies = collection_frequencies(prod_reviews, parameters['clean_reviews'])\n","    log.info(\"Writing collection frequencies to %s\" % parameters['collection_frequencies_path'])\n","    write_to_file(parameters['collection_frequencies_path'], word_doc_frequencies)\n","else:\n","    log.info(\"Reloading collection frequencies from %s\" % parameters['collection_frequencies_path'])\n","    word_doc_frequencies = load_from_file(parameters['collection_frequencies_path'])\n","\n","print(word_doc_frequencies)\n","print(type(word_doc_frequencies))"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"WUsBEmrcW5iQ","colab_type":"code","outputId":"7815525d-951e-4f4e-bb7f-2cb37ed7f45a","executionInfo":{"status":"ok","timestamp":1585739438188,"user_tz":-480,"elapsed":720,"user":{"displayName":"Mengxuan Tan","photoUrl":"","userId":"14268668782165182665"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(word_doc_frequencies)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["139844"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Kc3GBXzwIiAj"},"source":["### Tokenize Words & Calculate Word Frequency \n","\n","Only calculate word frequencies of non-stopwords\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rCQG3vlPEhcG","colab":{}},"source":["def _word_frequency(review) -> dict: \n","  global stopwords \n","  word_frequencies = {}  \n","  for token in nlp.make_doc(review):   #tokenize\n","      if token.text not in stopwords:\n","          if token.text not in word_frequencies.keys():\n","              word_frequencies[token.text] = 1\n","          else:\n","              word_frequencies[token.text] += 1\n","  return word_frequencies\n","\n","# wf = _word_frequency(prod_reviews['reviewText'][8])\n","# print(wf)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wmPUlAY9JFmy"},"source":["### Tokenize Sentences & Score Sentences"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"i3YejV3xJPXY","colab":{}},"source":["# def _tokenize_sentences(review) -> dict:\n","#   global sentnlp\n","#   doc = sentnlp(review)\n","#   sentence_dict = {}\n","#   for _, sentence in enumerate(doc.sents): \n","#     tokenized_sent = re.sub(r'[^\\w\\s]','', str(sentence))\n","#     if tokenized_sent is not None:\n","#       sentence_dict[tokenized_sent.strip()] = len(nlp.make_doc(sentence.text))\n","#   return sentence_dict\n","\n","# # sent_dict = _tokenize_sentences(prod_reviews['reviewText'][8])\n","# # print([(key, value) for key, value in sent_dict.items()])\n","\n","# # def _score_sentences(sentences:dict, word_frequencies:dict) -> dict:\n","# #     \"\"\"Score sentences based on word frequencies\"\"\"\n","# #     sentence_scores = {}\n","\n","# #     for sent, word_count_in_sent in sentences.items():\n","# #         for word_freq in word_frequencies:\n","# #             if word_freq in sent.lower():\n","# #                 if sent in sentence_scores: # use first 10 char as key\n","# #                     sentence_scores[sent] += word_frequencies[word_freq]\n","# #                 else:\n","# #                     sentence_scores[sent] = word_frequencies[word_freq]\n","        \n","# #         if sent in sentence_scores: # divide sentence score by word count to reduce advantages of long sentences\n","# #           sentence_scores[sent] = sentence_scores[sent] / word_count_in_sent\n","\n","# #     return sentence_scores\n","\n","# # sent_scores = _score_sentences(sent_dict, wf)\n","# # print([(key, value) for key, value in sent_scores.items()])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bYD6RBtouxr3"},"source":["### Sentence-level Summarizer\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1585739455320,"user_tz":-480,"elapsed":1141,"user":{"displayName":"Mengxuan Tan","photoUrl":"","userId":"14268668782165182665"}},"id":"yaC4c1728taN","outputId":"939da088-7253-429f-de76-d2b2935bd39f","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["  index = 10\n","  processedSentences = eval(prod_reviews['processedSentences'][index])\n","  reviewSentences = eval(prod_reviews['reviewSentences'][index])\n","\n","  print(len(processedSentences))\n","  print(len(reviewSentences))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["22\n","22\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"x74pUnehm1U2","colab":{}},"source":["from nltk.tokenize import sent_tokenize\n","\n","from numpy import dot\n","from numpy.linalg import norm\n","\n","def cos_sim(a, b):\n","  return dot(a, b)/(norm(a)*norm(b))\n","\n","def generate_summary(prod_reviews, index, num_of_docs, word_doc_frequencies, num_of_sentences):\n","    processedSentences = eval(prod_reviews['processedSentences'][index])\n","    reviewSentences = eval(prod_reviews['reviewSentences'][index])\n","\n","    ## generate word frequencies in cleaned\n","    cleaned_reviews = ' '.join(str(x) for x in processedSentences) \n","    word_frequencies = _word_frequency(cleaned_reviews)\n","\n","    ## generate review vector of len of word frequencies\n","    vocab = len(word_frequencies)\n","    # log.debug(\"vocab: %i\" % vocab)\n","    review_tf_vec = np.zeros(len(word_frequencies))\n","    for idx, word in enumerate(word_frequencies):\n","        tfidf_value = (1 + np.log(word_frequencies[word])) * np.log(num_of_docs/word_doc_frequencies[word]) \n","        review_tf_vec[idx] = tfidf_value\n","\n","    # print(review_tf_vec)\n","    log.debug(len(review_tf_vec))\n","\n","    ## generate sentences\n","    # log.debug(\"num of sentences: %i\" % len(processedSentences))\n","    # log.debug(\"sentences: %s\" % processedSentences)\n","\n","    ## generate sentence vectors of len of word frequencies\n","    sent_tf_vecs = []\n","    for sentence in processedSentences:\n","        sent_tf_vec = np.zeros(len(word_frequencies))\n","        sent_word_frequencies = _word_frequency(sentence)\n","        for idx, word in enumerate(sent_word_frequencies):\n","            if word in word_frequencies.keys():\n","                idx = list(word_frequencies.keys()).index(word)\n","                tfidf_value = (1 + np.log(sent_word_frequencies[word])) * np.log(num_of_docs/word_doc_frequencies[word]) \n","                sent_tf_vec[idx] = tfidf_value\n","        sent_tf_vecs.append(sent_tf_vec)\n","        # print('shape', sent_tf_vec.shape)\n","\n","    ## compare cossim between review vector and sentence vectors\n","    cosine_similarity = []\n","    for sent_tf_vec in sent_tf_vecs:\n","        cosine_similarity.append(cos_sim(review_tf_vec, sent_tf_vec))\n","\n","    highest_cos_idx = np.argsort(cosine_similarity)[-num_of_sentences:]\n","    # log.debug(\"highest_cos_idx: %s\" % str(highest_cos_idx))\n","\n","    ## original sentences\n","    # log.debug(\"original_sentences: %s\" % reviewSentences)\n","    # log.debug(\"num of ori sentences: %s\" % len(reviewSentences))\n","    summary_sentences = []\n","    for idx in highest_cos_idx:\n","        # log.debug(\"idx: %i ori: %s cleaned: %s\" % (idx, reviewSentences[idx], processedSentences[idx]))\n","        # log.debug(\"cos_vec: %s\" % str(sent_tf_vecs[idx]))\n","        summary_sentences.append(reviewSentences[idx])\n","\n","    return \" \".join(summary_sentences), summary_sentences\n","\n","# index=12\n","# summary, summary_sent = generate_summary(prod_reviews, index=index, num_of_docs=len(prod_reviews), word_doc_frequencies=word_doc_frequencies, num_of_sentences=3)\n","# log.info(\"===========================================================================================================================\")\n","# log.info(\"SUMMARIZED: %s\" % summary)\n","# log.info(\"ORIGINAL: %s\" % prod_reviews['reviewText'][index])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hzWf2niq2NjN","colab_type":"code","colab":{}},"source":["###### Kmeans algorithm ################\n","from sklearn.cluster import KMeans\n","from sklearn.metrics.pairwise import euclidean_distances\n","\n","def kmeans_summary(prod_reviews, index, num_of_docs, word_doc_frequencies, num_of_sentences):\n","\n","    processedSentences = eval(prod_reviews['processedSentences'][index])\n","    reviewSentences = eval(prod_reviews['reviewSentences'][index])\n","\n","    ## generate word frequencies in cleaned\n","    cleaned_reviews = ' '.join(str(x) for x in processedSentences) \n","    word_frequencies = _word_frequency(cleaned_reviews)\n","\n","    sent_tf_vecs = []\n","    for sentence in processedSentences:\n","        sent_tf_vec = np.zeros(len(word_frequencies))\n","        sent_word_frequencies = _word_frequency(sentence)\n","        for idx, word in enumerate(sent_word_frequencies):\n","            if word in word_frequencies.keys():\n","                idx = list(word_frequencies.keys()).index(word)\n","                tfidf_value = (1 + np.log(sent_word_frequencies[word])) * np.log(num_of_docs/word_doc_frequencies[word]) \n","                sent_tf_vec[idx] = tfidf_value\n","        sent_tf_vecs.append(sent_tf_vec)\n","\n","    est = KMeans(n_clusters=num_of_sentences, random_state=0).fit(np.array(sent_tf_vecs))\n","\n","    closest_centers_idx = []\n","    indexes = np.arange(len(sent_tf_vecs))\n","    cluster_ids = est.labels_\n","    centroids = est.cluster_centers_\n","\n","    # calculate each sentence's dist from its corresponding centroid \n","    dist_from_centroids = []\n","    for idx in range(len(sent_tf_vecs)):\n","        dist = euclidean_distances(sent_tf_vecs[idx].reshape(1,-1), centroids[cluster_ids[[idx]], :])\n","        dist_from_centroids.append(dist[0][0])\n","\n","    dist_arr = np.concatenate([np.array([dist_from_centroids]), cluster_ids.reshape(1,-1), indexes.reshape(1,-1)], axis = 0 )\n","    dist_arr = dist_arr.T\n","\n","    # find sentences with shortest dist to centroids\n","    for cluster_id in range(len(centroids)):\n","        dist_arr_id = dist_arr[dist_arr[:, 1] == cluster_id]\n","        row_idx = dist_arr_id[:, 0].argmin()\n","        sent_idx = dist_arr_id[row_idx][-1]\n","        closest_centers_idx.append(sent_idx)\n","    closest_centers_idx = sorted(closest_centers_idx)\n","    # print('SELECTED SENT index:', closest_centers_idx)\n","\n","    # print('\\n')\n","    # print('[ORIGINAL]:')\n","    # for sent in reviewSentences:\n","    #     print(sent)\n","\n","    #Comparing against vector space model\n","    # _, vec_sent = generate_summary(prod_reviews, index=index, num_of_docs=len(prod_reviews), word_doc_frequencies=word_doc_frequencies, num_of_sentences=num_of_sentences)\n","    # print('\\n')\n","    # print('[SUMMARY Vector Space]:')\n","    # for i in vec_sent:\n","    #     print(i)\n","\n","    summary_sent = []\n","    # print('\\n')\n","    # print('[SUMMARY Kmeans]:')\n","    for idx in closest_centers_idx:\n","        # print(reviewSentences[int(idx)])\n","        summary_sent.append(reviewSentences[int(idx)])\n","    \n","    return summary_sent"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iRKX5kTdE3q4","colab_type":"code","colab":{}},"source":["##############preprocessing for stru pos summary#####################################\n","\n","data = pd.read_json('CellPhoneReview.json', lines=True)\n","prod_ids  = data['asin'].values\n","reviewTexts  = data['reviewText'].values\n","unique_id = data['asin'].unique()\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","sentnlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner', 'tagger'])\n","sentnlp.add_pipe(nlp.create_pipe('sentencizer'))\n","punctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~©'\n","\n","def cleanup_text(text, stopwords, punc):\n","    texts = []\n","    global sentnlp\n","    doc = sentnlp(text) # only do tokenization and pos tagging\n","    for sentence in doc.sents:\n","        sen_text = nlp(sentence.text, disable=['parser', 'ner'])\n","        tokens = [tok.lemma_.lower().strip() for tok in sen_text if tok.lemma_ != '-PRON-']\n","        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punc]\n","        tokens = ' '.join(tokens)\n","        texts.append(tokens)\n","    return str(texts)\n","\n","def get_pos_tag(sent, noun_dict):\n","    '''identify nouns and adjs in each sent'''\n","\n","    nouns = []\n","    adj = []\n","    for tok in sent:\n","#         print(tok, tok.pos_, tok.lemma_)\n","        if str(tok.pos_) == 'NOUN':\n","            try:\n","                noun_dict[str(tok.lemma_)] += 1\n","            except:\n","                noun_dict[str(tok.lemma_)] = 1\n","            nouns.append(str(tok.lemma_))\n","        elif str(tok.pos_) == 'ADJ':\n","            adj.append(str(tok.lemma_))\n","        \n","    return nouns, adj"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z2Be_HO6G92Q","colab_type":"code","colab":{}},"source":["def get_tf_idf(sentences):\n","    \"get tf-idf vectors for sentences\"\n","    num_of_docs = len(unique_id)\n","    sent_tf_vecs = []\n","    for sentence in sentences:\n","        sent_tf_vec = np.zeros(len(word_frequencies))\n","        sent_word_frequencies = _word_frequency(sentence)\n","        for idx, word in enumerate(sent_word_frequencies):\n","            idx = all_words_ls.index(word)\n","            tfidf_value = (1 + np.log(sent_word_frequencies[word])) * np.log(num_of_docs/word_doc_frequencies[word]) \n","            sent_tf_vec[idx] = tfidf_value\n","        sent_tf_vecs.append(sent_tf_vec)\n","        \n","    return sent_tf_vecs\n","\n","def get_center_vec(sent_tf_vecs):\n","    \"get central sentence representation\"\n","    est = KMeans(n_clusters=1, random_state=0).fit(np.array(sent_tf_vecs))\n","\n","    closest_centers_idx = []\n","    indexes = np.arange(len(sent_tf_vecs))\n","    cluster_ids = est.labels_\n","    centroids = est.cluster_centers_\n","\n","    # calculate each sentence's dist from its corresponding centroid \n","    dist_from_centroids = []\n","    for idx in range(len(sent_tf_vecs)):\n","        dist = euclidean_distances(sent_tf_vecs[idx].reshape(1,-1), centroids[cluster_ids[[idx]], :])\n","        dist_from_centroids.append(dist[0][0])\n","\n","    dist_arr = np.concatenate([np.array([dist_from_centroids]), cluster_ids.reshape(1,-1), indexes.reshape(1,-1)], axis = 0 )\n","    dist_arr = dist_arr.T\n","\n","    # find sentences with shortest dist to centroids\n","    for cluster_id in range(len(centroids)):\n","        dist_arr_id = dist_arr[dist_arr[:, 1] == cluster_id]\n","        row_idx = dist_arr_id[:, 0].argmin()\n","        sent_idx = dist_arr_id[row_idx][-1]\n","        closest_centers_idx.append(sent_idx)\n","\n","    print('SELECTED SENT index:', closest_centers_idx)\n","\n","    best_vec = sent_tf_vecs[int(closest_centers_idx[0])]\n","    \n","    return best_vec, int(closest_centers_idx[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvkCAnqGkQdk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6f5fe316-9a6d-4c7a-85f6-97f1901650e6","executionInfo":{"status":"ok","timestamp":1585741825313,"user_tz":-480,"elapsed":1180,"user":{"displayName":"Mengxuan Tan","photoUrl":"","userId":"14268668782165182665"}}},"source":["np.mean(best_adj_tf_vecs[0])"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.02235676470497069"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"lWUhU2MJGNCF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b64d54a5-6725-4c15-bdf9-b6a0138393db","executionInfo":{"status":"ok","timestamp":1585742653621,"user_tz":-480,"elapsed":1689,"user":{"displayName":"Mengxuan Tan","photoUrl":"","userId":"14268668782165182665"}}},"source":["#choose review index\n","#index 90 has issue\n","# index = 5\n","index = 3\n","\n","chosen_sent_idx = []\n","\n","ori_sent = []\n","processed_sent = []\n","last_sent = []\n","nouns_master = []\n","adj_master = []\n","len_adj = []\n","\n","uni_id = unique_id[index]\n","\n","noun_dict = dict()\n","for idx in range(len(prod_ids)):\n","    if prod_ids[idx] == uni_id:\n","        text = reviewTexts[idx]\n","        sent_master = []\n","        doc = nlp(text)\n","        for sent in doc.sents:\n","#             print(sent)\n","            ori_sent.append(str(sent))\n","            clean_sent = cleanup_text(str(sent),stopwords, punctuations)\n","            processed_sent.append(eval(clean_sent)[0])\n","            nouns_list, adj_list = get_pos_tag(sent, noun_dict)\n","            nouns_master.append(nouns_list)\n","            adj_master.append(adj_list)\n","            len_adj.append(len(adj_list))\n","            last_sent.append(0)\n","#         print('\\n')\n","        last_sent[-1] = 1\n","    else:\n","        continue\n","        \n","sentence_frame = pd.DataFrame({'ori_sent': ori_sent, 'processed_sent':processed_sent, 'nouns':nouns_master,\n","                 'adj':adj_master, 'len_adj': len_adj, 'last_sent':last_sent, 'idx_sent': np.arange(len(ori_sent))})\n","                              \n","                              \n","# get word freq of all words in reviews\n","cleaned_reviews = ' '.join(str(x) for x in list(sentence_frame['processed_sent'].values)) \n","word_frequencies = _word_frequency(cleaned_reviews)\n","all_words_ls = list(word_frequencies.keys())\n","                              \n","########################### GET  1st SENTENCE (best sent that represents most occuring noun) #############################################\n","# find sentences with the most occuring noun\n","max_noun = max(noun_dict.items(), key=operator.itemgetter(1))[0]\n","clean_text_max_nouns = []\n","max_nouns_idx = []\n","for idx in range(len(sentence_frame)):\n","    if max_noun in sentence_frame['nouns'][idx]:\n","        clean_text_max_nouns.append(sentence_frame['processed_sent'][idx])\n","        max_nouns_idx.append(sentence_frame['idx_sent'][idx])\n","        \n","print('max_noun', max_noun)\n","                              \n","# construct tfidf of sentences with max noun\n","max_noun_tf_vecs = get_tf_idf(clean_text_max_nouns)\n","                              \n","#get central rep sent for max noun\n","bext_noun_vec, best_noun_sent_idx = get_center_vec(max_noun_tf_vecs)\n","chosen_idx = max_nouns_idx[best_noun_sent_idx]\n","chosen_sent_idx.append(chosen_idx)\n","                              \n","\n","################################################### GET  2nd and 3rg SENTENCE #############################################\n","#############################################sents that contain adj and are similar to 1st sent###############################\n","# get best adj sents\n","clean_text_best_adjs = []\n","best_adjs_idx = []\n","for idx in range(len(sentence_frame)):\n","    if idx not in chosen_sent_idx and len(sentence_frame['adj'][idx]) >= 1 :\n","        clean_text_best_adjs.append(sentence_frame['processed_sent'][idx])\n","        best_adjs_idx.append(sentence_frame['idx_sent'][idx])\n","                              \n","# construct tfidf of sentences with best adj\n","best_adj_tf_vecs = get_tf_idf(clean_text_best_adjs)\n","                              \n","# compute cos sim between best_adj_tf_vec and max_noun_vec, also compute mean tfidf score for each sent\n","adj_sim_dict = dict()\n","adj_tfidf_dict = dict()\n","for idx in range(len(best_adj_tf_vecs)):\n","    sim = cos_sim(best_adj_tf_vecs[idx], bext_noun_vec)\n","    adj_sim_dict[sim] = best_adjs_idx[idx]\n","    tfidf_score = np.mean(best_adj_tf_vecs[idx])\n","    adj_tfidf_dict[tfidf_score] = best_adjs_idx[idx]\n","\n","\n","#get idx of 2 sents with best scores  \n","for i in range(2):\n","    key = sorted(adj_sim_dict.keys())[-(i+1)]\n","    chosen_sent_idx.append(adj_sim_dict[key])\n","                              \n","################################################### GET  4th SENTENCE based on pure tfidf score #############################################\n","################################################### This is to add some diversity/details to the summary###################################\n","while True:\n","    i = -1\n","    key = sorted(adj_tfidf_dict.keys())[i]\n","    idx = adj_tfidf_dict[key]\n","    if idx in chosen_sent_idx:\n","        i -= 1\n","    else:\n","        chosen_sent_idx.append(idx)\n","        break\n","################################################### GET LAST SENTENCE #############################################\n","################################################### Most representative concluding sentence#######################################\n","\n","# get last sents\n","last_sents = []\n","last_sent_idx = []\n","for idx in range(len(sentence_frame)):\n","    if idx not in chosen_sent_idx and sentence_frame['last_sent'][idx] == 1 :\n","        last_sents.append(sentence_frame['processed_sent'][idx])\n","        last_sent_idx.append(sentence_frame['idx_sent'][idx])\n","                              \n","#get central rep sent for best last sentence\n","last_sents_tf_vecs = get_tf_idf(last_sents)\n","_, best_last_sent_idx = get_center_vec(last_sents_tf_vecs)\n","chosen_idx = last_sent_idx[best_last_sent_idx]\n","chosen_sent_idx.append(chosen_idx)\n","                              \n","print('\\n')\n","print('[ORIGINAL]:')\n","for i in sentence_frame['ori_sent'].values:\n","      print(i)\n","\n","num_of_sentences = 5\n","_, summary_vector_space = generate_summary(prod_reviews, index=index, num_of_docs=len(prod_reviews), word_doc_frequencies=word_doc_frequencies, num_of_sentences=num_of_sentences)\n","summary_kmeans = kmeans_summary(prod_reviews, index=index, num_of_docs=len(prod_reviews), word_doc_frequencies=word_doc_frequencies, num_of_sentences=num_of_sentences)\n","\n","print('\\n')\n","print('[VECTOR SPACE SUMMARY]:')\n","for i in summary_vector_space:\n","    print(i)\n","\n","print('\\n')\n","print('[KMEANS SUMMARY]:')\n","for i in summary_kmeans:\n","    print(i)\n","\n","print('\\n')\n","print('[STRUC POS SMMUARY]:')\n","for idx in chosen_sent_idx:\n","    if idx == chosen_sent_idx[-1]:\n","        last_sent = sentence_frame['ori_sent'][idx]\n","        last_sent = last_sent[0].lower() + last_sent[1:]\n","        print('Overall, ' + sentence_frame['ori_sent'][idx])\n","    else:\n","        print(sentence_frame['ori_sent'][idx])"],"execution_count":51,"outputs":[{"output_type":"stream","text":["max_noun case\n","SELECTED SENT index: [0.0]\n","SELECTED SENT index: [5.0]\n","\n","\n","[ORIGINAL]:\n","I love this case!\n","It's so pretty.\n","And I love the way the case feels to the touch because of the rubber.\n","Very happy!\n","The idea of the design is a sweet idea but it wears off as the paint is over the case and not under a sealant or anything.\n","It has a cool 3D effect but at the cost of the paint rubbing off.\n","It is quite pretty though.\n","I was worried about this order because the picture on the description page kept changing to a less-desireable green/orange cover.  \n","But alas, it came a day earlier than expected, and the beautiful pink product expected!  \n","The \"rubberized\" cover feels a little greasy (like it was armor-all'd), but the design is gorgeous and a little 3D looking!\n","Another pretty phone case that I really love.\n","I have many as I love\n","t o change them around .thanks\n","The case is not white, its more of a silver.\n","It's still a very pretty case-\n","it fit my phone perfectly.\n","It took too long to arrive.\n","Defintiely worth the price...\n","You should order several.  \n","Most cases are $15 and up especially at the cell phone stores.  \n","I can't believe the mark up on these things!\n","This is a great case...\n","It fit my phone perfectly and I still use it from time to time.\n","Would recommend\n","Received this item very quickly.\n","The design is even more vivid than expected.\n","The cover is soft (rubberized) but durable.\n","I have received many compliments.\n","It was an excellent buy!\n","I would recommend this to anyone wanting a \"good look\" for their phone.\n","this cover makes an old phone look and feel new.\n","I like that I can order covers for little money and snazzy up my phone.\n","[DEBUG] 87\n","\n","\n","[VECTOR SPACE SUMMARY]:\n","It has a cool 3D effect but at the cost of the paint rubbing off.\n","I have many as I love t o change them around .thanks The case is not white, its more of a silver.\n","I was worried about this order because the picture on the description page kept changing to a less-desireable green/orange cover.\n","The idea of the design is a sweet idea but it wears off as the paint is over the case and not under a sealant or anything.\n"," The \"rubberized\" cover feels a little greasy (like it was armor-all'd), but the design is gorgeous and a little 3D looking!\n","\n","\n","[KMEANS SUMMARY]:\n","I love this case!\n","The idea of the design is a sweet idea but it wears off as the paint is over the case and not under a sealant or anything.\n","I was worried about this order because the picture on the description page kept changing to a less-desireable green/orange cover.\n"," The \"rubberized\" cover feels a little greasy (like it was armor-all'd), but the design is gorgeous and a little 3D looking!\n","I have many as I love t o change them around .thanks The case is not white, its more of a silver.\n","\n","\n","[STRUC POS SMMUARY]:\n","I love this case!\n","I have many as I love\n","Another pretty phone case that I really love.\n","The \"rubberized\" cover feels a little greasy (like it was armor-all'd), but the design is gorgeous and a little 3D looking!\n","Overall, Would recommend\n"],"name":"stdout"}]}]}